---
title: "Considerations for choosing windows"
author: "Eamon O'Dea"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
bibliography: ews.bib
vignette: >
  %\VignetteIndexEntry{Choosing windows}
  %\VignetteEngine{knitr::rmarkdown_notangle}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

## Summary of goals

Illustrate how autocorrelation of moving window statistics may be calculated and how this can help determine an appropriate window size for correlation analysis.

## Simulation of data

First let's simulate some surveillance data from the lead-up to an outbreak.
The population size and length of the simulation are chosen to match California in the lead up to the 1991 measles outbreak, but we are mostly just going for a simulation from a simple conceptual model.

```{r sim}

tab <- data.frame(
  gamma_t = c(0, 0), mu_t = c(0, 0), d_t = c(0, 0), eta_t = c(0, 0), 
  beta_par_t = c(0, 1.4e-6 * 10 / 7), p_t = c(0, 0), time = c(0, 10))
foo <- spaero::create_simulator(covar = tab)
pomp::coef(foo)["N_0"] <- 1e7
pomp::coef(foo)["beta_par"] <- 1e-6
pomp::coef(foo)["eta"] <- 3e-6
out <- pomp::simulate(foo, times = seq(0, 10, by = 1 / 52), seed = 1)
out <- as(out, "data.frame")
plot(cases~time, data = out, type = 'h')
```


Clearly, a major outbreak is starting around year 9. Is there warning in the data from the first 7 years?

```{r}
cts <- ts(out$cases, frequency = 52)
sub <- window(cts, end = 7)
plot(sub)

```

## Null model

CSD would manifest in some change in the summary statistics of the time series. A simple null model is that the process generating the data is stationary. We would like to know the distribution of the summary statistics under such a null model to decide if the behavior of the statistics in the observed data is unlikely under such a model.

Stationary time series may be generated by the stationary bootstrap procedure. @politis1994 show that a block bootstraping procedure with geometric block lengths produces a stationary distribution. 

To simulate this data, we must decide on the average block size. @politis1994 decide on this in an example section by setting it equal to about twice the lag at which the autocovariance is not significantly different from zero.

```{r}
acf(sub)
```


According to that rule, an average block size of about 0.6 years seems appropriate for this data. We will use the boot::tsboot() function to resample data according to the stationary bootstrap.


```{r}
set.seed(3)
resamp <- boot::tsboot(sub, identity, R = 300, l = 52 * 0.5, sim = "geom")$t
plot(ts(t(resamp[1:6,]), frequency = 52))
```

The resampled time series look similar to the original in terms of autocorrelation. Now let's estimate the Fourier transform for our resampled time series. We will be able to use to calculate the autocovariance of the summary statistics.

```{r check-basic-fourier-transform-comprehension, echo = FALSE}

foo <- resamp[1,]
r1t <- fft(foo)
N <- length(r1t)
stopifnot(isTRUE(all.equal(Re(fft(r1t, inverse = TRUE) / N), resamp[1,])))

H <- function(N, cvec){
  n <- seq(0, N - 1)
  M <- length(cvec)
  nfreq <- n / N
  ret <- complex(N)
  for (ind in seq_len(M)) {
      ret <- ret + cvec[ind] * exp(-2i * pi * n[ind] * nfreq)
  }
  ret
}

R <- c(1, 1, rep(0, 311)) / 2
filtwind2 <- H(313, c(0.5, 0.5))
stopifnot(isTRUE(all.equal(fft(R), filtwind2)))

Ma2t <- r1t * fft(R)

stopifnot(isTRUE(all.equal(Mod(Ma2t), 
                           Mod(r1t) * Mod(fft(R))))) # PSD is scaled by PSD of respone function

ma2time <- Re(fft(Ma2t, inverse = TRUE)) / N
stopifnot(isTRUE(all.equal(ma2time, 
                           c(foo[N], foo[-N])/2 + c(foo) / 2)))

stopifnot(isTRUE(all.equal(zapsmall(ma2time),
                           zapsmall(convolve(resamp[1,], c(0.5, rep(0, 311), 0.5))))))


# convolution in frequency maps to multiplication in time:

stopifnot(isTRUE(all.equal(zapsmall(Re(fft(convolve(Ma2t, Ma2t) / N^2, inverse = TRUE))),
          zapsmall(ma2time^2))))

```

Let us begin with the mean.

```{r}

N <- dim(resamp)[2]

H <- function(N, cvec){
  n <- seq(0, N - 1)
  M <- length(cvec)
  nfreq <- n / N
  ret <- complex(N)
  for (ind in seq_len(M)) {
      ret <- ret + cvec[ind] * exp(-2i * pi * n[ind] * nfreq)
  }
  ret
}

ma2vec <- rep(1, 2) / 2
filtwind2 <- H(N, ma2vec)

resampT <- apply(resamp, 1, fft)
resampPSD <- Re(rowMeans(resampT * Conj(resampT))) / N ^2
resampacov <- Re(fft(resampPSD, inverse = TRUE))
resampMa2T <- resampT * filtwind2
resampMa2PSD <- Re(rowMeans(resampMa2T * Conj(resampMa2T))) / N ^ 2
resampMa2acov <- Re(fft(resampMa2PSD, inverse = TRUE))

plot(resampMa2acov[1:30], xlab = "lag", ylab = "autocovariance", ylim = c(0, 15))

resampMa2 <- apply(resampMa2T, 2, function(x) zapsmall(Re(fft(x, inverse = TRUE) / N)))
tmpf <- function(x) acf(x, type = "covariance", plot = FALSE, demean = FALSE)$acf[,1,1]
points(rowMeans(apply(resampMa2, 2, tmpf)), pch = 3, col = 2)
legend("topright", lty = c(1, NA, NA), pch = c(NA, 1, 3), col = c(1, 1, 2), legend = c("PSD method, window=1", "PSD method, window=2", "time-domain method, window=2"))
lines(resampacov)

```

The autocovariance appears to be significant for lags up to 20 in the moving average of 2 observations, not much longer than is the case for resmampled data that is not smoothed at all. Let's look at the effect of the length of the window a bit more systematically.

```{r}

N <- dim(resamp)[2]

H <- function(N, cvec){
  n <- seq(0, N - 1)
  M <- length(cvec)
  nfreq <- n / N
  ret <- complex(N)
  for (ind in seq_len(M)) {
      ret <- ret + cvec[ind] * exp(-2i * pi * n[ind] * nfreq)
  }
  ret
}

makemafilt <- function(filtN){
  rep(1, filtN) / filtN
}

filtNseq <- seq(1, 150)
filtvecs <- lapply(filtNseq, makemafilt)
filtwinds <- lapply(filtvecs, H, N = N)

resampMaT <- lapply(filtwinds, function(x) resampT * x)
resampMaPSD <- lapply(resampMaT, function(x) Re(rowMeans(x * Conj(x))) / N ^ 2)
resampMaAcov <- sapply(resampMaPSD, function(x) Re(fft(x, inverse = TRUE)))

matplot(y = resampMaAcov[1:50,], type = 'l', lty = 1, col = 1, xlab = "Lag", ylab = "Autocovariance", log = 'y')

persp(resampMaAcov[1:50,], xlab = "Lag", ylab = "Window", zlab = "Autocovariance", theta = 45)
image(log(resampMaAcov[1:50,]), x = 1:50, y = 1:ncol(resampMaAcov), xlab = "Lag", ylab = "Window")
plot(resampMaAcov[1:155, 110])
plot(resampMaAcov[1:155, 150])

```


Lengthening the window leads to a rapid drop in small-lag autocovariance and a smaller increase in long-range autocovariance. The autocovariance becomes almost triangular for large windows, with a kink in the function occuring at about the length of the window. 

Apparently, we are seeing the autocovariance become dominated by the extend of window overlap for sufficiently large windows. That is, it is so dominated when windows are sufficiently larger on the timescale of the intrinsic fluctuations in the timeseries.

Based on this view, two principles for the choice of the window are apparent. First, the window should be several times the time scale of the fluctuations in the time series due to intrinsic noise. Otherwise, the moving window statistics are subject to fluctuations themselved due to the stochastic effect that they are supposed to be characterizing. Second, any correlation analysis of the moving window statistics should occur on a timescale that is several times the length of the window size. Otherwise, trends in the moving window statistics will be dominated by covariance that may be attributed to overlap in windows. 

If the first principle is violated, but not the second, one is likely to estimate low correlations even when the time series is non-stationary. If the second principle is violated, one is likely to estimate large correlations in all cases. We will now provide examples of both of these problems, beginning with the second.

## Large correlations due to window overlap

We have already simulated many stationary time series using the stationary bootstrap. In these simulations, there is no real change in the mean over time. Therefore, correlation analyses that produce large estimates of the trend in the mean are not using a very efficient method. We will see how the window size of the correlation analysis effects the efficieny.

```{r}


invT <- function(mat) apply(mat, 2, function(x) zapsmall(Re(fft(x, inverse = TRUE) / N)))
resampMa <- lapply(resampMaT, invT)

get_tau <- function(x){
  if (any(is.finite(x))){
    stats::cor(x = seq_along(x), y = x, method = "kendall",
               use = "complete.obs")
  } else {
    NA
  }
}

window_tau <- function(ewsts, w) {
    suppressWarnings(zoo::rollapplyr(ewsts, width = w, FUN = get_tau, partial = FALSE))
}

mts_cor <- function(mts, w) apply(mts[, 1:10], 2, FUN = window_tau, w = w)

wseq <- seq(2, 150, 10)
cors <- Map(mts_cor, resampMa[wseq], w = wseq)

corvecs <- lapply(cors, as.numeric)
cordfs <- Map(function(x, y) data.frame(tau = x, window = y), corvecs, y = as.list(wseq))
plotdata <- na.omit(do.call(rbind, cordfs))

library(ggplot2)
library(ggridges)

g <- ggplot(data = plotdata, aes(x = tau, y = as.factor(window))) + geom_density_ridges() + ylab("Correlation window")
g

```


Even for very large window sizes, conducting a correlation analysis with the same-sized window leads to correlations that are somewhat extreme, when in fact the trend in the mean is known to be zero.

Now let's try a factor of 5 increase in the correlation window relative to the estimate of the mean.

```{r}

wseq5x <- seq(2, 150, 10) * 5
test <- wseq5x < N
cors5x <- Map(mts_cor, resampMa[wseq[test]], w = wseq5x[test])

corvecs5x <- lapply(cors5x, as.numeric)
cordfs5x <- Map(function(x, y) data.frame(tau = x, window = y), corvecs5x, y = as.list(wseq5x[test]))
plotdata5x <- na.omit(do.call(rbind, cordfs5x))

library(ggplot2)
library(ggridges)

g5x <- ggplot(data = plotdata5x, aes(x = tau, y = as.factor(window))) + geom_density_ridges() + ylab("Correlation window")
g5x

```

With a correlation analysis occuring on a larger timescale than the window, the correlations are less extreme, and become concentrated on the known value of zero for larger window sizes.

## Disclaimer

The content is solely the responsibility of the authors and does not
necessarily reflect the official views of the National Institutes of
Health.

## References

